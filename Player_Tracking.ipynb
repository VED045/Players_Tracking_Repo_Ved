{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HFRjMTT8k4fm"
      },
      "outputs": [],
      "source": [
        "# Option 2: Re-Identification in a Single Feed - Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V4M2UEtyl1tJ"
      },
      "outputs": [],
      "source": [
        "# Install YOLOv8 and dependencies for tracking\n",
        "!pip install -U ultralytics opencv-python-headless lap\n",
        "\n",
        "# Optional: For plotting, progress bars, and basic ML utils\n",
        "!pip install -q matplotlib pandas tqdm scikit-learn\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "\n",
        "print(\"\\n‚úÖ All packages installed. If this is your first run, go to Runtime ‚Üí Restart runtime NOW ‚ö†Ô∏è\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aG5yKZEwmFBG",
        "outputId": "aebd219a-f40c-4994-b10b-844a13894171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive Mounted Successfully!\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Google Drive Mounted Successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxodoQyfvJfd"
      },
      "source": [
        "**Step 3: Run Ultra-Accurate Player Tracking**\n",
        "This step performs robust player tracking using a custom tracking pipeline built on top of YOLOv8 detections. It includes:\n",
        "\n",
        "üîß What Happens in This Step:\n",
        "\n",
        "\n",
        "*   Load the trained YOLOv8 model (best.pt) from Google Drive.\n",
        "\n",
        "\n",
        "*   Read the input match video (15sec_input_720p.mp4) from Google Drive.\n",
        "\n",
        "\n",
        "*   Use YOLOv8 with ByteTrack to get detections and temporary tracker IDs.\n",
        "\n",
        "\n",
        "*   Apply a custom Re-Identification (Re-ID) pipeline that assigns consistent Player IDs using:\n",
        "\n",
        "\n",
        "1.   Deep Features extracted from ResNet50\n",
        "\n",
        "2.   Color Histograms\n",
        "\n",
        "3.   Dominant Colors\n",
        "\n",
        "4.   Bounding Box IoU\n",
        "\n",
        "5.   Motion Prediction\n",
        "\n",
        "\n",
        "\n",
        "*   Draw bounding boxes and persistent player IDs on each frame with visual   confidence levels.\n",
        "\n",
        "*   Save:\n",
        "\n",
        "üìπ A fully annotated output video (ultra_accurate_tracking.avi)\n",
        "\n",
        "üñºÔ∏è Each annotated frame as images inside /Outputs/tracked_frames/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JSX4LuB_olyX"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# ULTRA-ACCURATE PLAYER TRACKING SYSTEM\n",
        "# ==========================================\n",
        "# This system combines YOLO object detection with advanced tracking techniques\n",
        "# to maintain consistent player identities across video frames with minimal ID switching\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50\n",
        "import torch.nn as nn\n",
        "from collections import defaultdict, deque\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.cluster import DBSCAN\n",
        "import math\n",
        "\n",
        "# ==========================================\n",
        "# FILE PATHS AND SETUP\n",
        "# ==========================================\n",
        "# Define all necessary paths for model, input video, and output directory\n",
        "model_path = \"/content/drive/MyDrive/Player_Tracking/Models/best.pt\"\n",
        "video_path = \"/content/drive/MyDrive/Player_Tracking/Input_Video/15sec_input_720p.mp4\"\n",
        "output_dir = \"/content/drive/MyDrive/Player_Tracking/Outputs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ==========================================\n",
        "# YOLO MODEL INITIALIZATION\n",
        "# ==========================================\n",
        "# Load the pre-trained YOLO model for player detection\n",
        "# Using try-except for robust error handling during model loading\n",
        "try:\n",
        "    model = YOLO(model_path)\n",
        "    print(f\"‚úÖ Model loaded from: {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to load model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ==========================================\n",
        "# ENHANCED FEATURE EXTRACTOR CLASS\n",
        "# ==========================================\n",
        "# This class uses ResNet50 for deep feature extraction to create robust player representations\n",
        "# ResNet50 is chosen because it provides rich visual features that are excellent for person re-identification\n",
        "class EnhancedFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Use pre-trained ResNet50 as backbone for feature extraction\n",
        "        # Pre-trained weights help with better feature representation\n",
        "        self.backbone = resnet50(pretrained=True)\n",
        "        # Replace final classification layer with a feature vector of size 256\n",
        "        # This creates a compact but informative representation for each player\n",
        "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, 256)\n",
        "        self.backbone.eval()  # Set to evaluation mode for inference\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "# ==========================================\n",
        "# DEVICE SETUP AND MODEL INITIALIZATION\n",
        "# ==========================================\n",
        "# Use GPU if available for faster feature extraction, fallback to CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "feature_extractor = EnhancedFeatureExtractor().to(device)\n",
        "feature_extractor.eval()  # Ensure model is in evaluation mode\n",
        "\n",
        "# ==========================================\n",
        "# IMAGE PREPROCESSING PIPELINE\n",
        "# ==========================================\n",
        "# Standard ImageNet preprocessing pipeline for ResNet50\n",
        "# This ensures input images are in the format expected by the pre-trained model\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToPILImage(),           # Convert numpy array to PIL Image\n",
        "    transforms.Resize((224, 224)),     # Resize to standard input size for ResNet50\n",
        "    transforms.ToTensor(),             # Convert to tensor format\n",
        "    transforms.Normalize(              # Normalize with ImageNet statistics\n",
        "        mean=[0.485, 0.456, 0.406],   # ImageNet mean values\n",
        "        std=[0.229, 0.224, 0.225]     # ImageNet standard deviation values\n",
        "    )\n",
        "])\n",
        "\n",
        "# ==========================================\n",
        "# VIDEO INPUT/OUTPUT SETUP\n",
        "# ==========================================\n",
        "# Open input video and configure output video writer\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "if not cap.isOpened():\n",
        "    print(f\"‚ùå Error opening video file: {video_path}\")\n",
        "    exit()\n",
        "\n",
        "# Get video properties and set output to 30fps for consistency\n",
        "original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "output_video_path = os.path.join(output_dir, \"ultra_accurate_tracking.avi\")\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # XVID codec for good compression\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, 30.0, (width, height))  # Set to 30fps\n",
        "\n",
        "# Create directory for individual frames (useful for debugging and analysis)\n",
        "frames_dir = os.path.join(output_dir, \"tracked_frames\")\n",
        "os.makedirs(frames_dir, exist_ok=True)\n",
        "\n",
        "# ==========================================\n",
        "# ULTRA-ACCURATE PLAYER TRACKER CLASS\n",
        "# ==========================================\n",
        "# This is the core tracking system that maintains player identities across frames\n",
        "# It combines multiple techniques for maximum accuracy and minimal ID switching\n",
        "class UltraAccuratePlayerTracker:\n",
        "    def __init__(self, feature_extractor, device):\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.device = device\n",
        "\n",
        "        # ==========================================\n",
        "        # CORE TRACKING DATA STRUCTURES\n",
        "        # ==========================================\n",
        "        # These dictionaries maintain all player information across frames\n",
        "        self.tracker_to_player = {}     # Maps YOLO tracker IDs to our player IDs\n",
        "        self.player_features = {}       # Stores feature history for each player\n",
        "        self.player_boxes = {}          # Stores bounding box history for each player\n",
        "        self.player_positions = {}      # Stores center position history for each player\n",
        "        self.player_confidence = {}     # Confidence scores for each player assignment\n",
        "\n",
        "        # ==========================================\n",
        "        # SYSTEM CONFIGURATION\n",
        "        # ==========================================\n",
        "        # These parameters control the tracking behavior and accuracy\n",
        "        self.next_player_id = 1         # Counter for assigning new player IDs\n",
        "        self.max_players = 22           # Maximum expected players (soccer context)\n",
        "        self.feature_history_size = 400  # How many feature vectors to remember per player\n",
        "        self.position_history_size = 400 # How many positions to remember per player\n",
        "        self.box_history_size = 400     # How many bounding boxes to remember per player\n",
        "\n",
        "        # ==========================================\n",
        "        # MATCHING THRESHOLDS\n",
        "        # ==========================================\n",
        "        # These thresholds determine when to match detections to existing players\n",
        "        self.feature_similarity_threshold = 0.75  # Minimum feature similarity for matching\n",
        "        self.iou_threshold = 0.5                  # Minimum IoU for spatial matching\n",
        "        self.position_threshold = 100             # Maximum pixel distance for position matching\n",
        "        self.confidence_threshold = 0.6           # Minimum confidence for ID assignment\n",
        "\n",
        "        # ==========================================\n",
        "        # MISSING TRACKER MANAGEMENT\n",
        "        # ==========================================\n",
        "        # Handle cases where players temporarily disappear from detection\n",
        "        self.missing_trackers = {}      # Track how long each tracker has been missing\n",
        "        self.max_missing_frames = 20    # Maximum frames before removing a tracker\n",
        "\n",
        "        # ==========================================\n",
        "        # ID SWITCHING PREVENTION\n",
        "        # ==========================================\n",
        "        # Prevent rapid ID changes that can cause instability\n",
        "        self.id_switch_cooldown = {}    # Cooldown period after ID assignment\n",
        "        self.cooldown_frames = 10       # Frames to wait before allowing ID changes\n",
        "\n",
        "        # ==========================================\n",
        "        # MOTION PREDICTION\n",
        "        # ==========================================\n",
        "        # Store velocity information for predicting player movement\n",
        "        self.player_velocities = {}     # Velocity vectors for each player\n",
        "\n",
        "    def is_player_detection(self, box, class_id=None):\n",
        "        \"\"\"\n",
        "        PLAYER DETECTION FILTER\n",
        "        =====================\n",
        "        Filters out non-player detections (referees, balls, etc.) using heuristics\n",
        "        This is crucial for maintaining accuracy by only tracking actual players\n",
        "        \"\"\"\n",
        "        # Add your class filtering logic here\n",
        "        # If your model outputs class IDs, filter by player class\n",
        "        # For now, we'll use box size and position heuristics\n",
        "\n",
        "        x1, y1, x2, y2 = box\n",
        "        box_width = x2 - x1\n",
        "        box_height = y2 - y1\n",
        "        aspect_ratio = box_width / max(box_height, 1)\n",
        "\n",
        "        # Filter out very small detections (likely ball or false positives)\n",
        "        if box_width < 20 or box_height < 30:\n",
        "            return False\n",
        "\n",
        "        # Filter out very wide detections (likely not a person)\n",
        "        # Players should have a relatively normal human aspect ratio\n",
        "        if aspect_ratio > 2.0:\n",
        "            return False\n",
        "\n",
        "        # Filter out very tall detections (likely referee or wrong detection)\n",
        "        # Extremely tall/thin detections are usually errors\n",
        "        if aspect_ratio < 0.3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def extract_multi_features(self, frame, box):\n",
        "        \"\"\"\n",
        "        MULTI-MODAL FEATURE EXTRACTION\n",
        "        =============================\n",
        "        Extracts three types of features for robust player identification:\n",
        "        1. Deep features (ResNet50) - Most important for person re-identification\n",
        "        2. Color histogram - Captures uniform colors and patterns\n",
        "        3. Dominant color - Simple but effective for jersey identification\n",
        "        \"\"\"\n",
        "        x1, y1, x2, y2 = box\n",
        "\n",
        "        # Ensure coordinates are within frame bounds to prevent errors\n",
        "        x1 = max(0, min(x1, frame.shape[1]-1))\n",
        "        y1 = max(0, min(y1, frame.shape[0]-1))\n",
        "        x2 = max(x1+1, min(x2, frame.shape[1]))\n",
        "        y2 = max(y1+1, min(y2, frame.shape[0]))\n",
        "\n",
        "        roi = frame[y1:y2, x1:x2]  # Extract region of interest\n",
        "        if roi.size == 0:\n",
        "            return np.zeros(256), np.zeros(64), np.zeros(3)\n",
        "\n",
        "        # ==========================================\n",
        "        # 1. DEEP FEATURES USING RESNET50\n",
        "        # ==========================================\n",
        "        # These features capture complex visual patterns and are most important for identification\n",
        "        deep_features = np.zeros(256)\n",
        "        try:\n",
        "            roi_tensor = preprocess(roi).unsqueeze(0).to(self.device)\n",
        "            with torch.no_grad():  # Disable gradient computation for efficiency\n",
        "                deep_features = self.feature_extractor(roi_tensor).cpu().numpy().flatten()\n",
        "        except Exception as e:\n",
        "            print(f\"Deep feature extraction error: {e}\")\n",
        "\n",
        "        # ==========================================\n",
        "        # 2. COLOR HISTOGRAM FEATURES\n",
        "        # ==========================================\n",
        "        # Capture color distribution which is useful for jersey identification\n",
        "        color_features = np.zeros(64)\n",
        "        try:\n",
        "            roi_hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)  # HSV is better for color analysis\n",
        "            # Create 2D histogram for hue and saturation (8x8 bins)\n",
        "            hist = cv2.calcHist([roi_hsv], [0, 1], None, [8, 8], [0, 180, 0, 256])\n",
        "            color_features = hist.flatten() / (hist.sum() + 1e-7)  # Normalize histogram\n",
        "        except Exception as e:\n",
        "            print(f\"Color feature extraction error: {e}\")\n",
        "\n",
        "        # ==========================================\n",
        "        # 3. DOMINANT COLOR FEATURES\n",
        "        # ==========================================\n",
        "        # Simple but effective feature for jersey color identification\n",
        "        dominant_color = np.zeros(3)\n",
        "        try:\n",
        "            roi_resized = cv2.resize(roi, (64, 64))  # Resize for consistent processing\n",
        "            roi_flat = roi_resized.reshape(-1, 3)   # Flatten to get all pixels\n",
        "            dominant_color = np.mean(roi_flat, axis=0)  # Average color\n",
        "        except Exception as e:\n",
        "            print(f\"Dominant color extraction error: {e}\")\n",
        "\n",
        "        return deep_features, color_features, dominant_color\n",
        "\n",
        "    def compute_comprehensive_similarity(self, features1, features2):\n",
        "        \"\"\"\n",
        "        COMPREHENSIVE SIMILARITY COMPUTATION\n",
        "        ==================================\n",
        "        Combines multiple feature types with appropriate weighting\n",
        "        Deep features are weighted most heavily as they're most discriminative\n",
        "        \"\"\"\n",
        "        deep1, color1, dom1 = features1\n",
        "        deep2, color2, dom2 = features2\n",
        "\n",
        "        # ==========================================\n",
        "        # DEEP FEATURE SIMILARITY (MOST IMPORTANT)\n",
        "        # ==========================================\n",
        "        # Cosine similarity works well for normalized deep features\n",
        "        deep_sim = 0.0\n",
        "        if np.linalg.norm(deep1) > 0 and np.linalg.norm(deep2) > 0:\n",
        "            deep_sim = 1 - cosine(deep1, deep2)\n",
        "\n",
        "        # ==========================================\n",
        "        # COLOR HISTOGRAM SIMILARITY\n",
        "        # ==========================================\n",
        "        # Captures jersey color patterns\n",
        "        color_sim = 0.0\n",
        "        if np.linalg.norm(color1) > 0 and np.linalg.norm(color2) > 0:\n",
        "            color_sim = 1 - cosine(color1, color2)\n",
        "\n",
        "        # ==========================================\n",
        "        # DOMINANT COLOR SIMILARITY\n",
        "        # ==========================================\n",
        "        # Simple color matching for jersey identification\n",
        "        dom_sim = 0.0\n",
        "        if np.linalg.norm(dom1) > 0 and np.linalg.norm(dom2) > 0:\n",
        "            dom_sim = 1 - np.linalg.norm(dom1 - dom2) / (np.linalg.norm(dom1) + np.linalg.norm(dom2))\n",
        "\n",
        "        # ==========================================\n",
        "        # WEIGHTED COMBINATION\n",
        "        # ==========================================\n",
        "        # Deep features get highest weight (60%), color histogram (30%), dominant color (10%)\n",
        "        combined_sim = 0.6 * deep_sim + 0.3 * color_sim + 0.1 * dom_sim\n",
        "        return combined_sim, deep_sim, color_sim, dom_sim\n",
        "\n",
        "    def compute_iou(self, boxA, boxB):\n",
        "        \"\"\"\n",
        "        INTERSECTION OVER UNION (IoU) COMPUTATION\n",
        "        =======================================\n",
        "        Measures spatial overlap between bounding boxes\n",
        "        Essential for tracking continuity - players shouldn't jump around\n",
        "        \"\"\"\n",
        "        xA = max(boxA[0], boxB[0])\n",
        "        yA = max(boxA[1], boxB[1])\n",
        "        xB = min(boxA[2], boxB[2])\n",
        "        yB = min(boxA[3], boxB[3])\n",
        "\n",
        "        # Compute intersection area\n",
        "        interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "        if interArea == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # Compute union area\n",
        "        boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "        boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "\n",
        "        # IoU = intersection / union\n",
        "        return interArea / float(boxAArea + boxBArea - interArea)\n",
        "\n",
        "    def predict_position(self, player_id):\n",
        "        \"\"\"\n",
        "        MOTION PREDICTION\n",
        "        ================\n",
        "        Predicts where a player will be in the next frame based on velocity\n",
        "        Helps with tracking continuity when detections are temporarily lost\n",
        "        \"\"\"\n",
        "        if player_id not in self.player_positions or len(self.player_positions[player_id]) < 2:\n",
        "            return None\n",
        "\n",
        "        positions = list(self.player_positions[player_id])\n",
        "        if len(positions) < 2:\n",
        "            return positions[-1]\n",
        "\n",
        "        # Calculate velocity from last two positions\n",
        "        last_pos = positions[-1]\n",
        "        prev_pos = positions[-2]\n",
        "        velocity = (last_pos[0] - prev_pos[0], last_pos[1] - prev_pos[1])\n",
        "\n",
        "        # Predict next position using linear motion model\n",
        "        predicted_pos = (last_pos[0] + velocity[0], last_pos[1] + velocity[1])\n",
        "        return predicted_pos\n",
        "\n",
        "    def get_position_distance(self, box, predicted_pos):\n",
        "        \"\"\"\n",
        "        POSITION DISTANCE CALCULATION\n",
        "        ============================\n",
        "        Calculates distance between detection and predicted position\n",
        "        Used to maintain tracking continuity\n",
        "        \"\"\"\n",
        "        if predicted_pos is None:\n",
        "            return float('inf')\n",
        "\n",
        "        box_center = ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)\n",
        "        distance = math.sqrt((box_center[0] - predicted_pos[0])**2 +\n",
        "                           (box_center[1] - predicted_pos[1])**2)\n",
        "        return distance\n",
        "\n",
        "    def assign_player_id(self, tracker_id, box, features, frame_count):\n",
        "        \"\"\"\n",
        "        CORE PLAYER ID ASSIGNMENT LOGIC\n",
        "        ==============================\n",
        "        This is the heart of the tracking system. It decides which player ID\n",
        "        to assign to each detection using multiple criteria for maximum accuracy\n",
        "        \"\"\"\n",
        "\n",
        "        # ==========================================\n",
        "        # CHECK EXISTING TRACKER ASSIGNMENT\n",
        "        # ==========================================\n",
        "        # If tracker already has a player ID, verify it's still valid\n",
        "        if tracker_id in self.tracker_to_player:\n",
        "            player_id = self.tracker_to_player[tracker_id]\n",
        "\n",
        "            # Check if this ID is in cooldown (prevent rapid switching)\n",
        "            if player_id in self.id_switch_cooldown:\n",
        "                if frame_count - self.id_switch_cooldown[player_id] < self.cooldown_frames:\n",
        "                    # Update data and return existing ID\n",
        "                    self.update_player_data(player_id, box, features)\n",
        "                    return player_id\n",
        "                else:\n",
        "                    # Cooldown expired, remove it\n",
        "                    del self.id_switch_cooldown[player_id]\n",
        "\n",
        "            # Update player data and return existing assignment\n",
        "            self.update_player_data(player_id, box, features)\n",
        "            return player_id\n",
        "\n",
        "        # ==========================================\n",
        "        # MATCH WITH EXISTING PLAYERS\n",
        "        # ==========================================\n",
        "        # Try to match this detection with existing players using multiple criteria\n",
        "        best_matches = []\n",
        "\n",
        "        for player_id in self.player_features.keys():\n",
        "            # Skip if player is currently assigned to another active tracker\n",
        "            if player_id in self.tracker_to_player.values():\n",
        "                continue\n",
        "\n",
        "            # Get average features for comparison\n",
        "            if not self.player_features[player_id]:\n",
        "                continue\n",
        "\n",
        "            # Use recent features for comparison (last 20 feature vectors)\n",
        "            recent_features = list(self.player_features[player_id])[-20:]\n",
        "\n",
        "            # ==========================================\n",
        "            # COMPUTE FEATURE SIMILARITIES\n",
        "            # ==========================================\n",
        "            # Compare with multiple recent features to get best match\n",
        "            similarities = []\n",
        "            for stored_features in recent_features:\n",
        "                sim, deep_sim, color_sim, dom_sim = self.compute_comprehensive_similarity(\n",
        "                    features, stored_features)\n",
        "                similarities.append((sim, deep_sim, color_sim, dom_sim))\n",
        "\n",
        "            # Use best similarity from recent history\n",
        "            best_sim = max(similarities, key=lambda x: x[0])\n",
        "            combined_sim, deep_sim, color_sim, dom_sim = best_sim\n",
        "\n",
        "            # ==========================================\n",
        "            # COMPUTE SPATIAL OVERLAP (IoU)\n",
        "            # ==========================================\n",
        "            # Check if detection overlaps with recent player positions\n",
        "            iou = 0.0\n",
        "            if player_id in self.player_boxes and self.player_boxes[player_id]:\n",
        "                recent_boxes = list(self.player_boxes[player_id])[-5:]\n",
        "                ious = [self.compute_iou(box, stored_box) for stored_box in recent_boxes]\n",
        "                iou = max(ious)\n",
        "\n",
        "            # ==========================================\n",
        "            # POSITION PREDICTION SCORING\n",
        "            # ==========================================\n",
        "            # Score based on how close detection is to predicted position\n",
        "            predicted_pos = self.predict_position(player_id)\n",
        "            position_distance = self.get_position_distance(box, predicted_pos)\n",
        "            position_score = max(0, 1 - position_distance / self.position_threshold)\n",
        "\n",
        "            # ==========================================\n",
        "            # COMBINED SCORING\n",
        "            # ==========================================\n",
        "            # Combine all factors with appropriate weights\n",
        "            final_score = (0.5 * combined_sim +      # Feature similarity (most important)\n",
        "                          0.2 * iou +                # Spatial continuity\n",
        "                          0.2 * position_score +     # Motion prediction\n",
        "                          0.1 * self.player_confidence.get(player_id, 0))  # Historical confidence\n",
        "\n",
        "            # ==========================================\n",
        "            # QUALITY THRESHOLDS\n",
        "            # ==========================================\n",
        "            # Only consider matches that meet minimum quality requirements\n",
        "            if (combined_sim >= self.feature_similarity_threshold and\n",
        "                deep_sim >= 0.6 and  # Strong deep feature requirement\n",
        "                (iou >= self.iou_threshold or position_distance < self.position_threshold)):\n",
        "\n",
        "                best_matches.append((player_id, final_score, combined_sim, iou, position_distance))\n",
        "\n",
        "        # ==========================================\n",
        "        # SELECT BEST MATCH\n",
        "        # ==========================================\n",
        "        # Choose the best matching player from candidates\n",
        "        if best_matches:\n",
        "            best_matches.sort(key=lambda x: x[1], reverse=True)\n",
        "            best_match = best_matches[0]\n",
        "            player_id, score, sim, iou, pos_dist = best_match\n",
        "\n",
        "            # Additional verification for high confidence assignment\n",
        "            if score >= self.confidence_threshold:\n",
        "                self.tracker_to_player[tracker_id] = player_id\n",
        "                self.update_player_data(player_id, box, features)\n",
        "                # Increase confidence for successful matches\n",
        "                self.player_confidence[player_id] = min(1.0, self.player_confidence.get(player_id, 0) + 0.1)\n",
        "\n",
        "                # Set cooldown to prevent rapid switching\n",
        "                self.id_switch_cooldown[player_id] = frame_count\n",
        "\n",
        "                return player_id\n",
        "\n",
        "        # ==========================================\n",
        "        # ASSIGN NEW PLAYER ID\n",
        "        # ==========================================\n",
        "        # If no good match found and within player limit, create new player\n",
        "        if self.next_player_id <= self.max_players:\n",
        "            player_id = self.next_player_id\n",
        "            self.next_player_id += 1\n",
        "\n",
        "            self.tracker_to_player[tracker_id] = player_id\n",
        "            self.initialize_player_data(player_id, box, features)\n",
        "            self.player_confidence[player_id] = 0.5  # Initial confidence\n",
        "\n",
        "            return player_id\n",
        "\n",
        "        # No assignment possible (reached player limit or no good match)\n",
        "        return None\n",
        "\n",
        "    def update_player_data(self, player_id, box, features):\n",
        "        \"\"\"\n",
        "        UPDATE PLAYER DATA STRUCTURES\n",
        "        ============================\n",
        "        Maintains historical data for each player across frames\n",
        "        This history is crucial for accurate matching and tracking\n",
        "        \"\"\"\n",
        "        # Update feature history using deque for efficient memory management\n",
        "        if player_id not in self.player_features:\n",
        "            self.player_features[player_id] = deque(maxlen=self.feature_history_size)\n",
        "        self.player_features[player_id].append(features)\n",
        "\n",
        "        # Update bounding box history\n",
        "        if player_id not in self.player_boxes:\n",
        "            self.player_boxes[player_id] = deque(maxlen=self.box_history_size)\n",
        "        self.player_boxes[player_id].append(box)\n",
        "\n",
        "        # Update position history (using center points for motion analysis)\n",
        "        if player_id not in self.player_positions:\n",
        "            self.player_positions[player_id] = deque(maxlen=self.position_history_size)\n",
        "        center = ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)\n",
        "        self.player_positions[player_id].append(center)\n",
        "\n",
        "    def initialize_player_data(self, player_id, box, features):\n",
        "        \"\"\"\n",
        "        INITIALIZE NEW PLAYER DATA\n",
        "        =========================\n",
        "        Set up data structures for a newly detected player\n",
        "        \"\"\"\n",
        "        self.player_features[player_id] = deque([features], maxlen=self.feature_history_size)\n",
        "        self.player_boxes[player_id] = deque([box], maxlen=self.box_history_size)\n",
        "        center = ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)\n",
        "        self.player_positions[player_id] = deque([center], maxlen=self.position_history_size)\n",
        "\n",
        "    def update_missing_trackers(self, current_tracker_ids, frame_count):\n",
        "        \"\"\"\n",
        "        MISSING TRACKER MANAGEMENT\n",
        "        =========================\n",
        "        Handles cases where players temporarily disappear from detection\n",
        "        This is crucial for maintaining ID consistency across occlusions\n",
        "        \"\"\"\n",
        "        active_trackers = set(current_tracker_ids)\n",
        "\n",
        "        # Update missing counts for existing missing trackers\n",
        "        for tracker_id in list(self.missing_trackers.keys()):\n",
        "            if tracker_id in active_trackers:\n",
        "                # Tracker is back, remove from missing list\n",
        "                del self.missing_trackers[tracker_id]\n",
        "            else:\n",
        "                # Still missing, increment counter\n",
        "                self.missing_trackers[tracker_id] += 1\n",
        "\n",
        "        # Add newly missing trackers\n",
        "        for tracker_id in list(self.tracker_to_player.keys()):\n",
        "            if tracker_id not in active_trackers and tracker_id not in self.missing_trackers:\n",
        "                self.missing_trackers[tracker_id] = 1\n",
        "\n",
        "        # Clean up trackers that have been missing too long\n",
        "        to_remove = []\n",
        "        for tracker_id, missing_count in self.missing_trackers.items():\n",
        "            if missing_count > self.max_missing_frames:\n",
        "                to_remove.append(tracker_id)\n",
        "\n",
        "        # Remove long-missing trackers and reduce their confidence\n",
        "        for tracker_id in to_remove:\n",
        "            if tracker_id in self.tracker_to_player:\n",
        "                player_id = self.tracker_to_player[tracker_id]\n",
        "                # Reduce confidence for disappeared players\n",
        "                if player_id in self.player_confidence:\n",
        "                    self.player_confidence[player_id] = max(0, self.player_confidence[player_id] - 0.2)\n",
        "                del self.tracker_to_player[tracker_id]\n",
        "            del self.missing_trackers[tracker_id]\n",
        "\n",
        "# ==========================================\n",
        "# MAIN TRACKING LOOP INITIALIZATION\n",
        "# ==========================================\n",
        "# Initialize the ultra-accurate tracker and prepare for processing\n",
        "player_tracker = UltraAccuratePlayerTracker(feature_extractor, device)\n",
        "frame_count = 0\n",
        "\n",
        "print(\"\\nüéØ Starting ultra-accurate player tracking...\")\n",
        "print(\"üîç Features: Deep ResNet50 + Color + Position Prediction\")\n",
        "print(\"‚ö° Optimized for minimal ID switching\")\n",
        "\n",
        "# ==========================================\n",
        "# MAIN PROCESSING LOOP\n",
        "# ==========================================\n",
        "# Process each frame of the video with comprehensive tracking\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "    # ==========================================\n",
        "    # YOLO DETECTION AND TRACKING\n",
        "    # ==========================================\n",
        "    # Run YOLO with built-in ByteTrack tracker for initial detection and basic tracking\n",
        "    results = model.track(source=frame, persist=True, verbose=False, tracker=\"bytetrack.yaml\")\n",
        "    annotated_frame = frame.copy()\n",
        "\n",
        "    current_tracker_ids = []\n",
        "\n",
        "    # ==========================================\n",
        "    # PROCESS DETECTIONS\n",
        "    # ==========================================\n",
        "    # Extract detections and apply our enhanced tracking logic\n",
        "    if results[0].boxes.id is not None:\n",
        "        boxes = results[0].boxes.xyxy.cpu().numpy()\n",
        "        tracker_ids = results[0].boxes.id.cpu().numpy()\n",
        "\n",
        "        # ==========================================\n",
        "        # FILTER FOR PLAYER DETECTIONS ONLY\n",
        "        # ==========================================\n",
        "        # Remove non-player detections (referees, balls, etc.)\n",
        "        player_detections = []\n",
        "        for box, tracker_id in zip(boxes, tracker_ids):\n",
        "            if player_tracker.is_player_detection(box):\n",
        "                player_detections.append((box, int(tracker_id)))\n",
        "\n",
        "        current_tracker_ids = [tid for _, tid in player_detections]\n",
        "\n",
        "        # ==========================================\n",
        "        # PROCESS EACH PLAYER DETECTION\n",
        "        # ==========================================\n",
        "        for box, tracker_id in player_detections:\n",
        "            box = box.astype(int)\n",
        "            x1, y1, x2, y2 = box\n",
        "\n",
        "            # ==========================================\n",
        "            # EXTRACT COMPREHENSIVE FEATURES\n",
        "            # ==========================================\n",
        "            # Get multi-modal features for robust identification\n",
        "            features = player_tracker.extract_multi_features(frame, box)\n",
        "\n",
        "            # ==========================================\n",
        "            # ASSIGN PLAYER ID\n",
        "            # ==========================================\n",
        "            # Use our ultra-accurate matching system\n",
        "            player_id = player_tracker.assign_player_id(tracker_id, box, features, frame_count)\n",
        "\n",
        "            # ==========================================\n",
        "            # DRAW ENHANCED ANNOTATIONS\n",
        "            # ==========================================\n",
        "            # Color-coded visualization based on confidence levels\n",
        "            if player_id is not None:\n",
        "                confidence = player_tracker.player_confidence.get(player_id, 0)\n",
        "                if confidence > 0.7:\n",
        "                    color = (0, 255, 0)  # Green for high confidence\n",
        "                elif confidence > 0.4:\n",
        "                    color = (0, 255, 255)  # Yellow for medium confidence\n",
        "                else:\n",
        "                    color = (0, 165, 255)  # Orange for low confidence\n",
        "\n",
        "                label = f\"P{player_id}\"\n",
        "                conf_text = f\"{confidence:.2f}\"\n",
        "            else:\n",
        "                color = (0, 0, 255)  # Red for unassigned\n",
        "                label = \"?\"\n",
        "                conf_text = \"0.00\"\n",
        "\n",
        "            # Draw bounding box\n",
        "            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "            # Draw player label\n",
        "            cv2.putText(annotated_frame, label, (x1, y1-10),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
        "\n",
        "            # Draw confidence score\n",
        "            cv2.putText(annotated_frame, conf_text, (x1, y2+20),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "\n",
        "    # ==========================================\n",
        "    # UPDATE MISSING TRACKERS\n",
        "    # ==========================================\n",
        "    # Handle players that disappeared from current frame\n",
        "    player_tracker.update_missing_trackers(current_tracker_ids, frame_count)\n",
        "\n",
        "    # ==========================================\n",
        "    # ENHANCED FRAME INFORMATION\n",
        "    # ==========================================\n",
        "    # Display comprehensive tracking statistics\n",
        "    active_players = len(current_tracker_ids)\n",
        "    total_players = player_tracker.next_player_id - 1\n",
        "    info_text = f\"Frame: {frame_count} | Active: {active_players} | Total: {total_players}\"\n",
        "    cv2.putText(annotated_frame, info_text, (10, 30),\n",
        "               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "    # ==========================================\n",
        "    # OUTPUT GENERATION\n",
        "    # ==========================================\n",
        "    # Write frame to output video\n",
        "    out.write(annotated_frame)\n",
        "\n",
        "    # Save individual frame for analysis\n",
        "    frame_filename = os.path.join(frames_dir, f\"frame_{frame_count:04d}.jpg\")\n",
        "    cv2.imwrite(frame_filename, annotated_frame)\n",
        "\n",
        "    # ==========================================\n",
        "    # PROGRESS REPORTING\n",
        "    # ==========================================\n",
        "    # Regular progress updates every 30 frames\n",
        "    if frame_count % 30 == 0:\n",
        "        print(f\"üìä Frame {frame_count}: {active_players} players active, {total_players} total detected\")\n",
        "\n",
        "# ==========================================\n",
        "# CLEANUP AND FINAL RESULTS\n",
        "# ==========================================\n",
        "# Release resources and display final statistics\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "print(f\"\\n‚úÖ Ultra-accurate tracking complete!\")\n",
        "print(f\"üìÅ Output video: {output_video_path}\")\n",
        "print(f\"üìÅ Frames saved to: {frames_dir}\")\n",
        "print(f\"üéØ Total players: {player_tracker.next_player_id - 1}\")\n",
        "print(f\"üìà Final confidence scores: {player_tracker.player_confidence}\")\n",
        "print(f\"üîÑ ID mapping: {player_tracker.tracker_to_player}\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# BASIC OUTPUT VERSION (COMMENTED)\n",
        "# ==========================================\n",
        "# This is a simplified version for basic detection without advanced tracking\n",
        "# Uncomment this section if you want simple detection-only output\n",
        "\n",
        "# from ultralytics import YOLO\n",
        "# import cv2\n",
        "# import os\n",
        "\n",
        "# # ==========================================\n",
        "# # BASIC DETECTION-ONLY VERSION\n",
        "# # ==========================================\n",
        "# # This simplified version performs only detection without advanced tracking\n",
        "# # Useful for initial testing or when advanced tracking is not required\n",
        "\n",
        "# # === File paths ===\n",
        "# model_path = \"/content/drive/MyDrive/Player_Tracking/Models/best.pt\"\n",
        "# video_path = \"/content/drive/MyDrive/Player_Tracking/Input_Video/15sec_input_720p.mp4\"\n",
        "# output_dir = \"/content/drive/MyDrive/Player_Tracking/Outputs\"\n",
        "# frames_dir = os.path.join(output_dir, \"basic_tracked_frames\")\n",
        "\n",
        "# # === Create directories ===\n",
        "# # Ensure output directories exist for saving results\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "# os.makedirs(frames_dir, exist_ok=True)\n",
        "\n",
        "# # === Load YOLO model ===\n",
        "# # Load pre-trained YOLO model for basic detection\n",
        "# model = YOLO(model_path)\n",
        "# print(f\"‚úÖ Loaded model from: {model_path}\")\n",
        "\n",
        "# # === Load video ===\n",
        "# # Open input video file for processing\n",
        "# cap = cv2.VideoCapture(video_path)\n",
        "# if not cap.isOpened():\n",
        "#     print(\"‚ùå Error opening video file.\")\n",
        "#     exit()\n",
        "\n",
        "# # Get video properties for output configuration\n",
        "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "# width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "# height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# # === Set up video writer ===\n",
        "# # Configure output video with same properties as input\n",
        "# output_video_path = os.path.join(output_dir, \"basic_tracked_output.avi\")\n",
        "# fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Video codec\n",
        "# out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "# # === Process frames ===\n",
        "# # Main processing loop for basic detection\n",
        "# frame_id = 0\n",
        "# print(\"üöÄ Starting detection-only run...\")\n",
        "# while True:\n",
        "#     ret, frame = cap.read()\n",
        "#     if not ret:\n",
        "#         break\n",
        "\n",
        "#     # üëâ Run detection (no tracking)\n",
        "#     # This performs only object detection without maintaining identities\n",
        "#     results = model(frame)\n",
        "\n",
        "#     # üëâ Annotate detections on the frame\n",
        "#     # Draw bounding boxes and labels on detected objects\n",
        "#     annotated_frame = results[0].plot()\n",
        "\n",
        "#     # Optional: resize to original dimensions if needed\n",
        "#     annotated_frame = cv2.resize(annotated_frame, (width, height))\n",
        "\n",
        "#     # üëâ Write to video + save individual frames\n",
        "#     # Save both video output and individual frames\n",
        "#     out.write(annotated_frame)\n",
        "#     frame_path = os.path.join(frames_dir, f\"frame_{frame_id:04d}.jpg\")\n",
        "#     cv2.imwrite(frame_path, annotated_frame)\n",
        "\n",
        "#     print(f\"‚úÖ Processed frame {frame_id}\")\n",
        "#     frame_id += 1\n",
        "\n",
        "# # === Cleanup ===\n",
        "# # Release video capture and writer resources\n",
        "# cap.release()\n",
        "# out.release()\n",
        "\n",
        "# print(f\"\\nüéâ Detection complete.\")\n",
        "# print(f\"üìÅ Output video saved to: {output_video_path}\")\n",
        "# print(f\"üìÅ Frames saved in: {frames_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "End of Code.\n",
        "\n",
        "1. The Output video is In Outputs Folder named ultra_tracking.avi\n",
        "2. Frame by Frame output is saved in folder tracked_frames in output."
      ],
      "metadata": {
        "id": "z7D7bY37HKJS"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}